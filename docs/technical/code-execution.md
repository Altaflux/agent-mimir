# Code Execution Subsystem

## Scope

This document describes the Python execution path used by the code agent in:

- `agent-core/src/agent-manager/code-agent/`

It covers:

- prompt contract for code generation
- executor interface
- local and docker executors
- tool bridge protocol between Python and Node.js tools

## Execution contract

The code agent expects model output with XML tags:

- `<pip-dependencies-to-install>` comma-separated libs
- `<execution-code>` Python source

Parsing helpers:

- `getExecutionCodeContentRegex(...)`
- `getLibrariesContentRegex(...)`
- implemented in `agent-core/src/agent-manager/code-agent/utils.ts`

Prompt instructions are generated in:

- `agent-core/src/agent-manager/code-agent/prompt.ts`

## Executor interface

`CodeToolExecutor` (`agent-core/src/agent-manager/code-agent/index.ts`) requires:

- `availableDependencies: string[]`
- `workspaceFullPath(): string`
- `execute(tools, code, libraries, toolInitCallback): Promise<string>`

`workspaceFullPath()` is used in prompt text to tell the model where the current workspace is mounted.

## Local executor

`LocalPythonExecutor` (`executors/local-executor.ts`):

- creates one temp runtime directory with python venv
- reuses installed dependencies in executor instance memory
- per script:
  - writes generated python bootstrap script
  - starts Python process
  - waits for "INITIALIZED SERVER" marker
  - triggers tool bridge callback
  - captures output after initialization marker

## Docker executor

`DockerPythonExecutor` (`executors/docker-executor.ts`):

- container runtime strategy:
  - uses `docker run --rm` (ephemeral containers)
  - mounts runtime directory for venv/script reuse
  - mounts host workspace directory
  - maps websocket port for tool bridge
- venv/dependencies:
  - venv initialized once per executor instance/runtime dir
  - dependencies installed only when missing from executor cache
- workspace path in container:
  - defaults to `/workspace`
  - exposed to model via `workspaceFullPath()`
- default image:
  - `python:3.12`

### Logging and streamed container output

Current docker executor includes structured logs with prefix:

- `[DockerPythonExecutor] ...`

And per subprocess stream logging:

- `[DockerPythonExecutor][<label>][stdout|stderr] <line>`

This is controlled by option:

- `streamContainerOutput?: boolean` (default `true`)

## Python tool bridge protocol

Bridge components:

- Python websocket RPC server bootstrap generated by `python-code.ts`
- Node websocket client handler in `code-agent/tool-node.ts`

Flow:

1. Python process starts websocket RPC endpoint.
2. Node side connects when "INITIALIZED SERVER" is observed.
3. Python requests tool invocation by method name + args.
4. Node invokes matching `AgentTool`.
5. Node responds with result payload.
6. For non-JSON-schema tool outputs, Node stores output and returns marker:
   - `<<TOOL_RESPONSE:<call_id>>>`
7. After script completion, markers are expanded into final message content.

## Human approval integration

Before any code execution, the code agent graph pauses in `human_review_node`.

- accept -> execute script
- respond/edit -> converts feedback into tool message and loops back to LLM

## Operational caveats

- dependency cache is in-memory per executor instance plus persisted venv in runtime mount
- if process restarts, dependency install may run again depending on runtime directory lifecycle
- websocket-based tool bridge requires host port availability
- docker executor requires Docker daemon and CLI availability in runtime environment
